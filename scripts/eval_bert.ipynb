{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in open('datasets/train_1k.json', 'r'):\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/predictions.json',\"r\") as f:\n",
    "    preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict of each example_id with True/False based on whether it has a long answer\n",
    "def make_qid_to_has_ans(data):\n",
    "    qid_to_has_ans = {}\n",
    "    for entry in data:\n",
    "        qas_id = entry['example_id']\n",
    "        if entry['annotations'][0]['long_answer']['start_token']>=0 \\\n",
    "        and entry['annotations'][0]['long_answer']['end_token']>=0:\n",
    "            qid_to_has_ans[str(qas_id)] = True\n",
    "        else:\n",
    "            qid_to_has_ans[str(qas_id)] = False\n",
    "    return qid_to_has_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_scores(data, preds):\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "    for entry in data:\n",
    "        #find correct answer, empty string if there is no long answer\n",
    "        qas_id = entry['example_id']\n",
    "        if entry['annotations'][0]['long_answer']['start_token']>=0 \\\n",
    "        and entry['annotations'][0]['long_answer']['end_token']>=0:\n",
    "            long_answer_start= entry['annotations'][0]['long_answer']['start_token']\n",
    "            long_answer_end = entry['annotations'][0]['long_answer']['end_token']\n",
    "            para = entry['document_text']\n",
    "            split_text = para.split(' ')\n",
    "            doc_text = ' '.join([item for item in split_text[long_answer_start:long_answer_end] \\\n",
    "                                     if not ('<' in item or '>' in item) ])\n",
    "            a_gold = ' '\n",
    "            if len(entry['annotations'][0]['short_answers'])==0 and len(entry['annotations'][0]['long_answer'])!=0:\n",
    "                is_impossible=True\n",
    "            else:\n",
    "                is_impossible=False\n",
    "\n",
    "            if not is_impossible:\n",
    "                start_position = entry['annotations'][0]['short_answers'][0]['start_token']\n",
    "                end_position = entry['annotations'][0]['short_answers'][0]['end_token']\n",
    "                a_gold = ' '.join([item for item in split_text[start_position:end_position] \\\n",
    "                                         if not ('<' in item or '>' in item) ])\n",
    "                a_pred = preds[str(qas_id)]\n",
    "                exact_scores[str(qas_id)] = compute_exact(a_gold,a_pred)\n",
    "                f1_scores[str(qas_id)] = compute_f1(a_gold,a_pred)\n",
    "#                 print(a_gold)\n",
    "            else:\n",
    "                start_position = -1\n",
    "                end_position = -1\n",
    "                a_gold = \"\"\n",
    "                a_pred = preds[str(qas_id)]\n",
    "                exact_scores[str(qas_id)] = compute_exact(a_gold,a_pred)\n",
    "                f1_scores[str(qas_id)] = compute_f1(a_gold,a_pred)\n",
    "#             gold_answers = [orig_answer_text]\n",
    "        else:\n",
    "            a_gold=''\n",
    "#         if qas_id not in preds:\n",
    "#             print('Missing prediction for %s' % qas_id)\n",
    "#             continue\n",
    "#         a_pred = preds[qas_id]\n",
    "#         exact_scores[qas_id] = compute_exact(a_gold,a_pred)\n",
    "#         f1_scores[qas_id] = compute_f1(a_gold,a_pred)\n",
    "    return exact_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
    "    new_scores = {}\n",
    "    for qid, s in scores.items():\n",
    "        pred_na = na_probs[str(qid)] > na_prob_thresh\n",
    "        if pred_na:\n",
    "            new_scores[str(qid)] = float(not qid_to_has_ans[str(qid)])\n",
    "        else:\n",
    "            new_scores[str(qid)] = s\n",
    "    return new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict([('exact', 100.0 * sum(exact_scores.values()) / total),\\\n",
    "                                        ('f1', 100.0 * sum(f1_scores.values()) / total),('total', total),])\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "        return collections.OrderedDict([('exact', 100.0 * sum(exact_scores[str(k)] for k in qid_list) / total),\\\n",
    "                                        ('f1', 100.0 * sum(f1_scores[str(k)] for k in qid_list) / total),\\\n",
    "                                        ('total', total),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval(main_eval, new_eval, prefix):\n",
    "    for k in new_eval:\n",
    "        main_eval['%s_%s' % (prefix, str(k))] = new_eval[str(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(precisions, recalls, out_image, title):\n",
    "    plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title(title)\n",
    "    plt.savefig(out_image)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
    "                               out_image=None, title=None):\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "    true_pos = 0.0\n",
    "    cur_p = 1.0\n",
    "    cur_r = 0.0\n",
    "    precisions = [1.0]\n",
    "    recalls = [0.0]\n",
    "    avg_prec = 0.0\n",
    "    for i, qid in enumerate(qid_list):\n",
    "        if qid_to_has_ans[str(qid)]:\n",
    "            true_pos += scores[str(qid)]\n",
    "        cur_p = true_pos / float(i+1)\n",
    "        cur_r = true_pos / float(num_true_pos)\n",
    "        if i == len(qid_list) - 1 or na_probs[str(qid)] != na_probs[qid_list[i+1]]:\n",
    "            # i.e., if we can put a threshold after this point\n",
    "            avg_prec += cur_p * (cur_r - recalls[-1])\n",
    "            precisions.append(cur_p)\n",
    "            recalls.append(cur_r)\n",
    "    if out_image:\n",
    "        plot_pr_curve(precisions, recalls, out_image, title)\n",
    "    return {'ap': 100.0 * avg_prec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
    "                                  qid_to_has_ans, out_image_dir):\n",
    "    if out_image_dir and not os.path.exists(out_image_dir):\n",
    "        os.makedirs(out_image_dir)\n",
    "    num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
    "    if num_true_pos == 0:\n",
    "        return\n",
    "    pr_exact = make_precision_recall_eval(\n",
    "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
    "      title='Precision-Recall curve for Exact Match score')\n",
    "    pr_f1 = make_precision_recall_eval(\n",
    "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
    "      title='Precision-Recall curve for F1 score')\n",
    "    oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
    "    pr_oracle = make_precision_recall_eval(\n",
    "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
    "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
    "    merge_eval(main_eval, pr_exact, 'pr_exact')\n",
    "    merge_eval(main_eval, pr_f1, 'pr_f1')\n",
    "    merge_eval(main_eval, pr_oracle, 'pr_oracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
    "    if not qid_list:\n",
    "        return\n",
    "    x = [na_probs[str(k)] for k in qid_list]\n",
    "    weights = np.ones_like(x) / float(len(x))\n",
    "    plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
    "    plt.xlabel('Model probability of no-answer')\n",
    "    plt.ylabel('Proportion of dataset')\n",
    "    plt.title('Histogram of no-answer probability: %s' % name)\n",
    "    plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[str(k)])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[str(k)])\n",
    "    for i, qid in enumerate(qid_list):\n",
    "        if qid not in scores: continue\n",
    "        if qid_to_has_ans[str(qid)]:\n",
    "            diff = scores[str(qid)]\n",
    "        else:\n",
    "            if preds[str(qid)]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "        cur_score += diff\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[str(qid)]\n",
    "    return 100.0 * best_score / len(scores), best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "    main_eval['best_exact'] = best_exact\n",
    "    main_eval['best_exact_thresh'] = exact_thresh\n",
    "    main_eval['best_f1'] = best_f1\n",
    "    main_eval['best_f1_thresh'] = f1_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_probs = {k: 0.0 for k in preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_has_ans = make_qid_to_has_ans(data)  # maps qid to True/False\n",
    "exact_raw, f1_raw = get_raw_scores(data, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exact_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_raw, f1_raw = get_raw_scores(data, preds)\n",
    "exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,1.0)\n",
    "f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_eval = make_eval_dict(exact_thresh, f1_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 54.618473895582326),\n",
       "             ('f1', 58.290175942010926),\n",
       "             ('total', 498)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
