Actual data has 300k rows and is difficult to iterate fast <br>

Carving out *random* chunks of it for faster modelling  <br>

<br>
1k rows (50 MB) - train_1k.json <br>
5k rows (500 MB) - train_5k.json <br>
50k rows (2.5 GB) - train_50k.json <br>
<br>

Files can be downloaded at - https://drive.google.com/open?id=1nA1wzNqA78zAXjS8FFxtybK--CBSPBvU
