{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertLM_validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEGRKMxAmkB-",
        "colab_type": "code",
        "outputId": "a5a8bcde-08a1-49c9-cc32-baacbec46844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "#Google Colab and GPU Set UP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "!pip install pytorch_pretrained_bert\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertModel, BertForMaskedLM\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "nltk.download('punkt')\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.memory_allocated())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(device,n_gpu)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1+cu100)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 59.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.14)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.14->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "1.3.1+cu100\n",
            "0\n",
            "cuda 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4eEh0v5mbhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting File Path\n",
        "base_dir = \"drive/My Drive/\" #Loading google drive to work on colaboratory\n",
        "test_file = \"sampled_simplified_nq_30000_test_6000.json\" #Add your file name here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azlrFsUim4gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4751cbe-ee24-4237-821d-67b51e8dd0ac"
      },
      "source": [
        "filename = base_dir + test_file\n",
        "with open(filename) as json_file:\n",
        "    data = json.load(json_file)\n",
        "print(len(data))\n",
        "print(data[0].keys())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n",
            "dict_keys(['document_text', 'long_answer_candidates', 'question_text', 'annotations', 'document_url', 'example_id'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLt-ES_nm5w7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab10b8cd-0549-4973-f4fc-aa0e5928dfc1"
      },
      "source": [
        "def format_data_for_LM(data):\n",
        "  ''' Takes the raw data and converts into q_a format i.e. each example has a question + answer attribute '''\n",
        "  qa_data = []\n",
        "  c = 0\n",
        "  for i in range(len(tqdm(data))):\n",
        "    question = data[i]['question_text']\n",
        "    doc_text = data[i]['document_text']\n",
        "    split_text = doc_text.split(' ')\n",
        "    la_start = data[i]['annotations'][0]['long_answer']['start_token']\n",
        "    la_end = data[i]['annotations'][0]['long_answer']['end_token']\n",
        "    long_answer = ' '.join([item for item in split_text[la_start:la_end] if not ('<' in item or '>' in item) ])\n",
        "    \n",
        "    q_a = question+\" ? \"+long_answer\n",
        "    long_answer_candidate_texts = []\n",
        "    temp = {}\n",
        "    temp[\"answer_index\"] = -1\n",
        "    for long_an_can in data[i]['long_answer_candidates']:\n",
        "      lac_start = long_an_can['start_token']\n",
        "      lac_end = long_an_can['end_token']\n",
        "      if lac_start == la_start and lac_end == la_end:\n",
        "        temp[\"answer_index\"] = data[i]['long_answer_candidates'].index(long_an_can)\n",
        "\n",
        "      long_answer_candidate_text = ' '.join([item for item in split_text[lac_start:lac_end] if not ('<' in item or '>' in item) ])\n",
        "      long_answer_candidate_texts.append(long_answer_candidate_text)\n",
        "    \n",
        "    temp['example_id'] = data[i]['example_id']\n",
        "    temp['q_a'] = q_a\n",
        "    temp['question'] = question + \" ? \"\n",
        "    temp['long_answers'] = long_answer_candidate_texts\n",
        "    if long_answer == '':\n",
        "      temp[\"answer_present\"] = False\n",
        "    else:\n",
        "      temp[\"answer_present\"] = True\n",
        "    qa_data.append(temp)\n",
        "  return qa_data\n",
        "qa_data = format_data_for_LM(data)\n",
        "\n",
        "# print(long_answer)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEVwiH-npClJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0f0c844a-6a62-4647-ae2a-a56117ceafec"
      },
      "source": [
        "print(len(qa_data))\n",
        "print(qa_data[0].keys())\n",
        "print(qa_data[11]['answer_index'])\n",
        "print(qa_data[11]['answer_present'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n",
            "dict_keys(['answer_index', 'example_id', 'q_a', 'question', 'long_answers', 'answer_present'])\n",
            "39\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwuYgXf0nepM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Might save it as well as colab is fragile\n",
        "# base_dir = \"drive/My Drive/\"\n",
        "# data_file = \"qa_data.json\"\n",
        "# import json\n",
        "# filename = base_dir + data_file\n",
        "# with open(filename,\"w\") as json_file:\n",
        "#     json.dump(qa_data,json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVn4q-Vp_pT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get qa_data from the file\n",
        "# base_dir = \"drive/My Drive/\"\n",
        "# data_file = \"qa_data.json\"\n",
        "# filename = base_dir + data_file\n",
        "# print(filename)\n",
        "# import json\n",
        "# with open(filename) as json_file:\n",
        "#     qa_data = json.load(json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37KQe_3cbke2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "19786e41-4764-46ed-8759-cb4b7577e204"
      },
      "source": [
        "def filter_qa_data(qa_data):\n",
        "  filtered_qa_data = [item for item in qa_data if item['answer_present']]\n",
        "  return filtered_qa_data\n",
        "\n",
        "filtered_qa_data = filter_qa_data(qa_data)\n",
        "print(len(filtered_qa_data))\n",
        "print(filtered_qa_data[0].keys())\n",
        "print(len(filtered_qa_data[0]['long_answers']))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3009\n",
            "dict_keys(['answer_index', 'example_id', 'q_a', 'question', 'long_answers', 'answer_present'])\n",
            "57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7cR_gZbt7GY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "37cd521c-0744-488c-d728-4dc47e3b04dc"
      },
      "source": [
        "#Load the model\n",
        "# del model\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.cuda()\n",
        "# file_path = base_dir + \"dlt_bertLM_10000_q_a_paragraphs_model_3.pth\"\n",
        "# model.load_state_dict(torch.load(file_path))\n",
        "model.eval()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 36203755.36B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1182473.04B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rn6SIQ8aaFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_loader(tokenizer,data):\n",
        "\n",
        "  sentences = []\n",
        "  for data_point in data:\n",
        "    sentences += [data_point['question'] + la for la in data_point['long_answers']]\n",
        "  tok_sentences = sentences # q_a passed as one sequence\n",
        "  sentences_with_special_tokens = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in tok_sentences]\n",
        "\n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences_with_special_tokens]\n",
        "  print(\"Tokenization Done......\")\n",
        "  MAX_LEN = 512\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  inputs = torch.tensor(input_ids)\n",
        "  print(\"Padding Done......\")\n",
        "  # Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "  # with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "  tensor_data = TensorDataset(inputs)\n",
        "  # train_sampler = RandomSampler(train_data)\n",
        "  print(\"Calling Dataloader\")\n",
        "  dataloader = DataLoader(tensor_data, batch_size=batch_size, drop_last=False)\n",
        "  return dataloader,sentences_with_special_tokens\n",
        "\n",
        "dataloader,sentences_with_special_tokens = create_data_loader(tokenizer,filtered_qa_data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNje8jQhdv-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "36d7003c-d143-4a53-85cb-820752f314c1"
      },
      "source": [
        "# for item in sentences_with_special_tokens[:100]:\n",
        "#   print(item)\n",
        "# print(len(dataloader))\n",
        "# print(scores)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-02113a37a0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RztZ1eE5dzt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5de4803-d52f-408a-c013-6ad655dce24c"
      },
      "source": [
        "output = []\n",
        "temp = None\n",
        "scores = []\n",
        "for step, batch in enumerate(tqdm(dataloader, position=0, leave=True)):\n",
        "# Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids = batch[0]\n",
        "  # print(b_input_ids[0][-50:])\n",
        "  # print(sentences_with_special_tokens[0])\n",
        "  # print(\"Scores length is \",len(scores))\n",
        "  # print(b_input_ids)\n",
        "  # print(\"Input id shape\",b_input_ids.shape)\n",
        "  # print(\"HERE:\",b_input_ids, b_input_mask)\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "  # print(b_input_ids)\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    output = model(b_input_ids).cpu().numpy()\n",
        "    b_input_ids = b_input_ids.cpu().numpy()\n",
        "    # print(output.shape)\n",
        "    # print(b_input_ids.shape)\n",
        "    \n",
        "    for i in range(len(b_input_ids)):\n",
        "      c = 0\n",
        "      temp = []\n",
        "      for item in b_input_ids[i]:\n",
        "        if item:\n",
        "          temp.append(output[i,c,item])\n",
        "        c += 1\n",
        "      # print(len(temp),temp)\n",
        "      scores.append(np.sum(np.array(temp))/len(temp))\n",
        "\n",
        "\n",
        "    # print(len(output),output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 24736/24736 [4:01:38<00:00,  1.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMinM_HFBOXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82740ad6-11d4-4d89-9e30-70b0ef2bbbc4"
      },
      "source": [
        "print(len(scores))\n",
        "# print(scores[0:50])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "395773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAZ2uhWOAerm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bb22a22-5a18-403c-a988-f1360b62d544"
      },
      "source": [
        "print(filtered_qa_data[0].keys())\n",
        "start = 0\n",
        "end = 0\n",
        "for i in range(len(filtered_qa_data)):\n",
        "  # print(\"Number of long answers are {}\".format(len(filtered_qa_data[i]['long_answers'])))\n",
        "  end += len(filtered_qa_data[i]['long_answers'])\n",
        "  scr = scores[start:end]\n",
        "  filtered_qa_data[i]['scores'] = scr\n",
        "  start = end\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['answer_index', 'example_id', 'q_a', 'question', 'long_answers', 'answer_present'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmAwZ5TmB0yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"filtered_qa_6000.json\",\"w\") as f:\n",
        "  json.dump(filtered_qa_data,f)\n",
        "\n",
        "#The rest of the windows are just experiments."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcb9pcJgZedj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in filtered_qa_data:\n",
        "  # print(len(item['long_answers']))\n",
        "  if len(item['long_answers']) != len(item['scores']):\n",
        "    print(\"LOL\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnae8QqlUG24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "bd35d2e0-da0c-4b22-8c17-a733be339723"
      },
      "source": [
        "print(len(filtered_qa_data[1]['long_answers']),len(filtered_qa_data[1]['scores']))\n",
        "print(filtered_qa_data[1]['answer_index'])\n",
        "print(filtered_qa_data[1]['scores'][filtered_qa_data[1]['answer_index']])\n",
        "print(np.argsort(np.array(filtered_qa_data[1]['scores'])))\n",
        "print(filtered_qa_data[1].keys())\n",
        "print(np.min(np.array(filtered_qa_data[1]['scores'])), np.max(np.array(filtered_qa_data[1]['scores'])),np.median(np.array(filtered_qa_data[1]['scores'])))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "143 143\n",
            "93\n",
            "18.509088689630683\n",
            "[ 94  19  16   4   1   7  65 121 127 114 128 104 118 123  99   3  96  97\n",
            " 103 102  64 126 124  67 111 125   8 120 113  20 117 116 110 122  98 115\n",
            " 134  63 119 107 130 135   2 131 100  26 112 129 106 133 105 137 108 109\n",
            "  25 101  18  66  73   5 140  82 138 132  95 136  11  12  13   9 139  71\n",
            "  33  59  61  29  21  34  62  17  48   6  22  24  23 141  37  36  39  41\n",
            "  28  44  43   0  72  47  32  76  46  68  15  14  80  56  53  38  78  10\n",
            "  52  45  58 142  60  54  77  51  93  40  70  49  86  91  75  69  31  87\n",
            "  92  79  85  57  90  30  50  55  74  88  27  42  84  89  81  83  35]\n",
            "dict_keys(['answer_index', 'example_id', 'q_a', 'question', 'long_answers', 'answer_present', 'scores'])\n",
            "9.367835998535156 20.17344857865021 15.935251596811655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s41p8RiAWAAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "19a0d87b-f8fb-43c1-f3f8-a4ef74abce53"
      },
      "source": [
        "def top_k_precision(filtered_qa_data,k = 5, threshold = 15.00):\n",
        "  t = 0\n",
        "  p = 0\n",
        "  p_th = 0\n",
        "  count = 0\n",
        "  av_answer_length = 0\n",
        "  high = 0\n",
        "  low = 10000000\n",
        "  scrs = []\n",
        "  for sample in filtered_qa_data:\n",
        "    if sample['answer_index'] in np.flip(np.argsort(np.array(sample['scores'])))[:k]:\n",
        "      p += 1\n",
        "    t += 1\n",
        "    av_answer_length += len(sample['scores'])\n",
        "    if len(sample['scores']) > high:\n",
        "      high = len(sample['scores'])\n",
        "    if len(sample['scores']) < low:\n",
        "      low = len(sample['scores'])\n",
        "    scrs += sample['scores']\n",
        "    if sample['answer_index'] in np.argwhere(np.array(sample['scores']) >= threshold ).flatten():\n",
        "      p_th += 1\n",
        "    count += len(np.argwhere(np.array(sample['scores']) >= threshold ).flatten())\n",
        "\n",
        "\n",
        "  av_answer_length /= len(filtered_qa_data)\n",
        "  return float(p/t),av_answer_length,high,low,scrs, (p_th/t), (count/t)\n",
        "\n",
        "precision, av_answer_length,high_la_can,low_la_can,scrs, precision_threhold,av_la_picked = top_k_precision(filtered_qa_data,50,15)\n",
        "# output = top_k_precision(filtered_qa_data,50,15)\n",
        "print(precision, av_answer_length,high_la_can,low_la_can,scrs, precision_threhold,av_la_picked)\n",
        "\n",
        "print(\"Min,Max,Mean and Median of scores are...\")\n",
        "print(np.min(np.array(scrs)),np.max(np.array(scrs)),np.mean(np.array(scrs)),np.median(np.array(scrs)))\n",
        "print(len(filtered_qa_data))\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8368228647391159 131.52974410103025 1885 1 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvlyzxPaIlfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_length = len(tokenizer.tokenize(question))\n",
        "c_q_a = []\n",
        "c_q = []\n",
        "c_a = []\n",
        "q_sum = []\n",
        "a_sum = []\n",
        "q_a_sum = []\n",
        "for k in range(len(input_ids)):\n",
        "  temp_c_q_a = 0\n",
        "  temp_c_q = 0\n",
        "  temp_c_a = 0\n",
        "  temp_q_sum = 0.0\n",
        "  temp_a_sum = 0.0\n",
        "  temp_q_a_sum = 0.0\n",
        "  for i in range(1,len(input_ids[k])-1):\n",
        "    if (input_ids[k][i+1] != 0):\n",
        "      temp_c_q_a += 1\n",
        "      temp_q_a_sum += output[k][:,i,input_ids[k][i]][0].cpu().numpy()\n",
        "    if (i < 1+question_length ):\n",
        "      temp_c_q += 1\n",
        "      temp_q_sum += output[k][:,i,input_ids[k][i]][0].cpu().numpy()\n",
        "    elif (i >= 1+question_length and input_ids[k][i+1] != 0):\n",
        "      temp_c_a += 1\n",
        "      temp_a_sum += output[k][:,i,input_ids[k][i]][0].cpu().numpy()\n",
        "  if input_ids[k][-1] != 0:\n",
        "    temp_a_sum += output[k][:,i,input_ids[k][-1]][0].cpu().numpy()\n",
        "    temp_q_a_sum += output[k][:,i,input_ids[k][-1]][0].cpu().numpy()\n",
        "\n",
        "  c_q_a.append(temp_c_q_a)\n",
        "  c_q.append(temp_c_q)\n",
        "  c_a.append(temp_c_a)\n",
        "  q_sum.append(temp_q_sum)\n",
        "  a_sum.append(temp_a_sum)\n",
        "  q_a_sum.append(temp_q_a_sum)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nivjc-BvSjWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Question......\")\n",
        "print(q_sum)\n",
        "print(c_q)\n",
        "normalized_q_sum = np.array(q_sum)/np.array(c_q)\n",
        "print(np.argmin(q_sum),np.argmax(q_sum))\n",
        "print(np.argmin(normalized_q_sum),np.argmax(normalized_q_sum))\n",
        "print(\"Answers......\")\n",
        "print(a_sum)\n",
        "print(c_a)\n",
        "normalized_a_sum = np.array(a_sum)/np.array(c_a)\n",
        "print(np.argmin(a_sum),np.argmax(a_sum))\n",
        "print(np.argmin(normalized_a_sum),np.argmax(normalized_a_sum))\n",
        "print(\"Question+Answers.......\")\n",
        "print(q_a_sum)\n",
        "print(c_q_a)\n",
        "normalized_q_a_sum = np.array(q_a_sum)/np.array(c_q_a)\n",
        "print(np.argmin(q_a_sum),np.argmax(q_a_sum))\n",
        "print(np.argmin(normalized_q_a_sum),np.argmax(normalized_q_a_sum))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2kJvcZ9eVrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arg_sort = np.argsort(q_a_sum)\n",
        "# print(q_a_sum[36])\n",
        "# print(arg_sort)\n",
        "predictions = []\n",
        "scores = []\n",
        "for i in range(len(q_a_sum)-1,-1,-1):\n",
        "  # print(i,arg_sort[i])\n",
        "  if arg_sort[i] == 0:\n",
        "    predictions.append(sentences[arg_sort[i]])\n",
        "    scores.append(q_a_sum[arg_sort[i]])\n",
        "    break\n",
        "  else:\n",
        "    predictions.append(sentences[arg_sort[i]])\n",
        "    scores.append(q_a_sum[arg_sort[i]])\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  print(scores[i],predictions[i])\n",
        "# print(sentences[0])\n",
        "# print(sentences[16])\n",
        "# print(sentences[0])\n",
        "# print(q_a_sum[0],q_a_sum[16])\n",
        "# print(input_ids[0] == input_ids[16])\n",
        "# print(input_ids[16])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vxBSWrYd8Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the model\n",
        "# del model\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.cuda()\n",
        "file_path = base_dir + \"dlt_bertLM_10000_q_a_paragraphs_model_3.pth\"\n",
        "model.load_state_dict(torch.load(file_path))\n",
        "model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCy9U_e18uoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind = 8\n",
        "temp = qa_data[:100]\n",
        "top_score_ft = []\n",
        "correct_answer_score_ft = []\n",
        "position_ft = []\n",
        "\n",
        "for ind in range(len(temp)):\n",
        "  try:\n",
        "    # print(\"Testing Cuda Memory Usage\",torch.cuda.memory_allocated())\n",
        "    q_a = temp[ind]['q_a']\n",
        "    question = temp[ind]['question']\n",
        "    sentences = []\n",
        "    sentences.append(q_a)\n",
        "    for item in temp[ind]['long_answers']:\n",
        "      sentences.append(question+item)\n",
        "\n",
        "    # tok_sentences = sent_tokenize(sentences)\n",
        "    tok_sentences = sentences # q_a passed as one sequence\n",
        "    # print(tok_sentences)\n",
        "    sentences_with_special_tokens = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in tok_sentences]\n",
        "\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences_with_special_tokens]\n",
        "    MAX_LEN = 256\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    train_inputs = torch.tensor(input_ids)\n",
        "\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "      seq_mask = [float(i>0) for i in seq]\n",
        "      attention_masks.append(seq_mask)\n",
        "\n",
        "    train_masks = torch.tensor(attention_masks)\n",
        "\n",
        "\n",
        "\n",
        "    # Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "    batch_size = min(16,input_ids.shape[0])\n",
        "\n",
        "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "    # with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_masks)\n",
        "    # train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, drop_last=False)\n",
        "    # print(\"Testing Cuda Memory Usage\",torch.cuda.memory_allocated())\n",
        "\n",
        "    #Load the model\n",
        "    # del model\n",
        "    # model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "    # model.cuda()\n",
        "    # file_path = base_dir + \"dlt_bertLM_10000_q_a_paragraphs_model_3.pth\"\n",
        "    # model.load_state_dict(torch.load(file_path))\n",
        "    # model.eval()\n",
        "\n",
        "    # print(\"Testing Cuda Memory Usage\",torch.cuda.memory_allocated())\n",
        "    #Run Evaluation\n",
        "    output = []\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
        "      # print(\"inside eval\")\n",
        "    # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        temp_o = model(b_input_ids, attention_mask=b_input_mask)\n",
        "        for i in range(len(temp_o)):\n",
        "          output.append(temp_o[i])\n",
        "        del temp_o\n",
        "\n",
        "    # print(\"Output shape\",len(output),output[0].shape,output[0][5,7].cpu().numpy())\n",
        "    with torch.no_grad():\n",
        "      question_length = len(tokenizer.tokenize(question))\n",
        "      c_q_a = []\n",
        "      c_q = []\n",
        "      c_a = []\n",
        "      q_sum = []\n",
        "      a_sum = []\n",
        "      q_a_sum = []\n",
        "      for k in range(len(input_ids)):\n",
        "        temp_c_q_a = 0\n",
        "        temp_c_q = 0\n",
        "        temp_c_a = 0\n",
        "        temp_q_sum = 0.0\n",
        "        temp_a_sum = 0.0\n",
        "        temp_q_a_sum = 0.0\n",
        "        for i in range(1,len(input_ids[k])-1):\n",
        "          if (input_ids[k][i+1] != 0):\n",
        "            temp_c_q_a += 1\n",
        "            temp_q_a_sum += output[k][i,input_ids[k][i]].cpu().numpy()\n",
        "          if (i < 1+question_length ):\n",
        "            temp_c_q += 1\n",
        "            temp_q_sum += output[k][i,input_ids[k][i]].cpu().numpy()\n",
        "          elif (i >= 1+question_length and input_ids[k][i+1] != 0):\n",
        "            temp_c_a += 1\n",
        "            temp_a_sum += output[k][i,input_ids[k][i]].cpu().numpy()\n",
        "        if input_ids[k][-1] != 0:\n",
        "          temp_a_sum += output[k][i,input_ids[k][-1]].cpu().numpy()\n",
        "          temp_q_a_sum += output[k][i,input_ids[k][-1]].cpu().numpy()\n",
        "        \n",
        "        c_q_a.append(temp_c_q_a)\n",
        "        c_q.append(temp_c_q)\n",
        "        c_a.append(temp_c_a)\n",
        "        q_sum.append(temp_q_sum)\n",
        "        a_sum.append(temp_a_sum)\n",
        "        q_a_sum.append(temp_q_a_sum)\n",
        "      del output\n",
        "      # print(\"Question......\")\n",
        "      # print(question)\n",
        "      # print(\"Actual Answer......\")\n",
        "      # print(q_a)\n",
        "      normalized_q_sum = np.array(q_sum)/np.array(c_q)\n",
        "      normalized_a_sum = np.array(a_sum)/np.array(c_a)\n",
        "      normalized_q_a_sum = np.array(q_a_sum)/np.array(c_q_a)\n",
        "      per_difference = (normalized_q_a_sum-normalized_a_sum)*100/normalized_a_sum\n",
        "      # print(np.argsort(per_difference))\n",
        "      arg_sort = np.argsort(normalized_q_a_sum)\n",
        "      # arg_sort = np.argsort(per_difference)\n",
        "      predictions = []\n",
        "      scores = []\n",
        "      for i in range(len(normalized_q_a_sum)-1,-1,-1):\n",
        "        if arg_sort[i] == 0:\n",
        "          predictions.append(sentences[arg_sort[i]])\n",
        "          scores.append(normalized_q_a_sum[arg_sort[i]])\n",
        "          break\n",
        "        else:\n",
        "          predictions.append(sentences[arg_sort[i]])\n",
        "          scores.append(normalized_q_a_sum[arg_sort[i]])\n",
        "\n",
        "      # for i in range(len(predictions)):\n",
        "      #   print(scores[i],predictions[i])\n",
        "      top_score_ft.append(scores[0])\n",
        "      correct_answer_score_ft.append(scores[-1])\n",
        "      position_ft.append(len(scores))\n",
        "\n",
        "      print(\"Testing Cuda Memory Usage\",torch.cuda.memory_allocated())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ993bvw-59I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(top_score)\n",
        "print(correct_answer_score)\n",
        "print(position)\n",
        "print(np.mean(np.array(position)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOz7UdcF9_qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(top_score_ft)\n",
        "print(correct_answer_score_ft)\n",
        "print(position_ft)\n",
        "print(np.mean(np.array(position_ft)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}